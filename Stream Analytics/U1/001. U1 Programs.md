
# 1. Credit based flow control (Data Transfer)

![[Pasted image 20241222231955.png]]

### **Explanation of the Code**

The provided Python code demonstrates how to use **PyFlink** (Python API for Apache Flink) to implement a simple data stream processing application with **Credit-Based Flow Control**. Here's a detailed explanation of each part:

---

### **1. StreamExecutionEnvironment Initialization**

```python
from pyflink.datastream import StreamExecutionEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
```

- **`StreamExecutionEnvironment`**: This is the entry point for Flink's DataStream API. It sets up the execution environment for streaming applications.
- **`get_execution_environment()`**: Retrieves the default execution environment, which is used to configure and execute the data processing pipeline.

---

### **2. Buffer Timeout Configuration**

```python
env.set_buffer_timeout(100)
```

- **Buffer Timeout**:
    - Flink uses a **buffer** to temporarily hold data before transferring it to the next operator in the pipeline.
    - The `set_buffer_timeout(100)` configures the timeout to **100 milliseconds**. This means the buffer will hold data for a maximum of 100 ms before sending it downstream.
    - **Purpose**: Controls the trade-off between throughput and latency:
        - **Low Timeout**: Reduces latency by sending smaller batches of data quickly.
        - **High Timeout**: Increases throughput by batching more data before sending.

---

### **3. Data Source**

```python
data_stream = env.from_collection(["event1", "event2", "event3"])
```

- **`from_collection()`**: Creates a DataStream from an in-memory collection of data (`["event1", "event2", "event3"]`).
- **Data Stream**: Represents a continuous stream of data elements (`event1`, `event2`, `event3`).

---

### **4. Transformation**

```python
mapped_stream = data_stream.map(lambda x: x.upper())
```

- **`map()` Transformation**:
    - Applies a user-defined function to each element in the stream.
    - **`lambda x: x.upper()`**: Converts each string in the stream to uppercase (e.g., `event1` → `EVENT1`).

---

### **5. Sink**

```python
mapped_stream.print()
```

- **`print()`**: Sends the transformed data stream to a standard output (console).
- **Output Example**:
    
    ```
    EVENT1
    EVENT2
    EVENT3
    ```
    

---

### **6. Execute the Job**

```python
env.execute("Credit-Based Flow Control Example")
```

- **`execute()`**: Triggers the execution of the Flink job.
- The string `"Credit-Based Flow Control Example"` is used as the job name, which helps identify it in the Flink UI or logs.

---

# 2. Task Chainind - Disabled
![[Pasted image 20241222234346.png]]

### **Explanation of the Code**

The provided code demonstrates how **task chaining** in Apache Flink can be disabled to analyze its effect on the job execution. Here’s a detailed explanation of each part of the code:

---

### **1. StreamExecutionEnvironment Initialization**

```python
from pyflink.datastream import StreamExecutionEnvironment
import time
start = time.time()

env = StreamExecutionEnvironment.get_execution_environment()
```

- **`StreamExecutionEnvironment`**: The entry point for Flink’s DataStream API, used to set up the streaming environment.
- **`get_execution_environment()`**: Retrieves the execution environment for the pipeline.
- **`time.time()`**: Captures the start time to measure the execution duration.

---

### **2. Disabling Operator Chaining**

```python
env.disable_operator_chaining()
```

- **Operator Chaining**:
    - By default, Flink **chains compatible operators** together into a single task to optimize performance by reducing communication overhead between operators.
    - **Disabling Chaining**: Prevents Flink from combining tasks. Each operator (`map`, `filter`, etc.) will run as a separate task.
    - **Purpose**: Useful for debugging or when an operator’s behavior impacts downstream operators, and isolation is required.

---

### **3. Data Source**

```python
data_stream = env.from_collection([1, 2, 3, 4, 5])
```

- **`from_collection()`**: Creates a DataStream from an in-memory collection of integers `[1, 2, 3, 4, 5]`.
- **Data Stream**: Represents a continuous stream of data (here, integers).

---

### **4. Transformations**

```python
chained_stream = data_stream.map(lambda x: x * 2).filter(lambda x: x > 5)
```

- **`map()`**:
    
    - Applies the function `lambda x: x * 2` to each element in the stream.
    - Example: Input `[1, 2, 3, 4, 5]` → After `map()`: `[2, 4, 6, 8, 10]`.
- **`filter()`**:
    
    - Filters out elements that do not satisfy the condition `x > 5`.
    - Example: `[2, 4, 6, 8, 10]` → After `filter()`: `[6, 8, 10]`.
- **Chaining Behavior**:
    
    - When chaining is enabled (default), the `map` and `filter` operations would run as a single task.
    - With chaining disabled, `map` and `filter` are executed as separate tasks.

---

### **5. Sink**

```python
chained_stream.print()
```

- **`print()`**: Sends the transformed data to the standard output (console).
- **Output Example**:
    
    ```
    6
    8
    10
    ```
    

---

### **6. Execute the Job**

```python
env.execute("Task Chaining Example -- Disabled")
```

- **`execute()`**: Triggers the Flink job execution.
- The job name (`"Task Chaining Example -- Disabled"`) identifies it in the Flink UI or logs.

---

### **7. Time Measurement**

```python
end = time.time()
print(f'Time taken: {end - start}')
```

- **Time Measurement**:
    - Calculates the total time taken to execute the Flink job.
    - `end - start`: Measures the elapsed time from the beginning to the end of the script execution.

---

# 3. Task Chaining - Enabled
![[Pasted image 20241222234534.png]]


# 4. State > Keyed State

![[Pasted image 20241223010350.png]]

This Python code demonstrates the use of **keyed state** in PyFlink to maintain stateful information for elements in a keyed stream. Here’s a detailed breakdown:

---

### **1. Initialization of Stream Execution Environment**

```python
from pyflink.datastream import StreamExecutionEnvironment, KeyedProcessFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types

env = StreamExecutionEnvironment.get_execution_environment()
```

- **`StreamExecutionEnvironment`**:
    - Sets up the streaming environment for Flink.
    - Provides methods to define the data pipeline and execute the Flink job.
- **`KeyedProcessFunction`**: A user-defined function to process each element in a keyed stream while accessing and updating keyed state.

---

### **2. Data Source**

```python
data_stream = env.from_collection([("user1", 1), ("user2", 2), ("user1", 3)])
```

- **`from_collection()`**: Creates a DataStream from a Python list of tuples.
- **Input Data**:
    - A stream of tuples, where the first element is a user ID (`"user1"`, `"user2"`) and the second element is an event value.
    - Example: `[("user1", 1), ("user2", 2), ("user1", 3)]`.

---

### **3. Defining the KeyedProcessFunction**

```python
class CountPerKey(KeyedProcessFunction):

    def open(self, runtime_context):
        self.count_state = runtime_context.get_state(
            ValueStateDescriptor("count", Types.INT()))
```

- **`KeyedProcessFunction`**:
    
    - A function that processes elements in a keyed stream and provides access to state and time-related features.
- **`open(self, runtime_context)`**:
    
    - Called when the function is initialized.
    - Initializes a **keyed state variable** using a `ValueStateDescriptor`.
- **Keyed State**:
    
    - **`ValueStateDescriptor("count", Types.INT())`**: Defines a state variable named `"count"` of type `INT`. Each key (e.g., `"user1"`, `"user2"`) will maintain its own separate count value.

---

### **4. Processing Each Element**

```python
def process_element(self, value, ctx):
    current_count = self.count_state.value() or 0
    current_count += 1
    self.count_state.update(current_count)
    print(f"Key: {value[0]}, Count: {current_count}")
```

- **Parameters**:
    
    - `value`: The current element being processed (e.g., `("user1", 1)`).
    - `ctx`: Context object for accessing time-related information (unused here).
- **Steps**:
    
    1. **Retrieve the Current Count**:
        - `self.count_state.value()` retrieves the current count for the key. If no value exists (i.e., first encounter), it defaults to `0`.
    2. **Increment the Count**:
        - `current_count += 1` increments the count for the current key.
    3. **Update the State**:
        - `self.count_state.update(current_count)` stores the updated count back into the keyed state.
    4. **Print Key-Count Pair**:
        - Outputs the key (`value[0]`) and the updated count to the console for debugging purposes.

---

### **5. Key By Key**

```python
data_stream.key_by(lambda x: x[0]).process(CountPerKey())
```

- **`key_by(lambda x: x[0])`**:
    - Partitions the stream based on the first element of the tuple (the user ID, e.g., `"user1"`, `"user2"`).
    - All elements with the same key are processed by the same instance of the `CountPerKey` function.
- **`process(CountPerKey())`**:
    - Applies the `CountPerKey` function to the keyed stream.

---

### **6. Execute the Job**

```python
env.execute("Keyed State -- User Count")
```

- **`execute()`**:
    - Executes the Flink job with the name `"Keyed State -- User Count"`.

---

### **Output**

For the given input data:

```python
[("user1", 1), ("user2", 2), ("user1", 3)]
```

- Execution Flow:
    
    1. `("user1", 1)`:
        - Key: `"user1"`, Count: 1.
    2. `("user2", 2)`:
        - Key: `"user2"`, Count: 1.
    3. `("user1", 3)`:
        - Key: `"user1"`, Count: 2.
- Console Output:
    

```
Key: user1, Count: 1
Key: user2, Count: 1
Key: user1, Count: 2
```

---

### **Key Concepts Demonstrated**

1. **Keyed State**:
    
    - Each key maintains its own independent state variable (`count`).
    - Useful for stateful processing, e.g., counting events or maintaining user sessions.
2. **Partitioning**:
    
    - The `key_by` operation ensures that all events for a specific key are processed by the same task.
3. **State Isolation**:
    
    - The count for one key (e.g., `"user1"`) does not affect the count for another key (e.g., `"user2"`).

<hr><hr>


# 7.
![[Pasted image 20241223012537.png]]

Below are the implementations of the two problems in PyFlink:

---

### **Problem 1: Word Count with Keyed State**

#### Description:

Count the occurrences of each word in a stream using a `KeyedProcessFunction` to maintain a `ValueState` for each word.

```python
from pyflink.datastream import StreamExecutionEnvironment, KeyedProcessFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types

# Initialize the Stream Execution Environment
env = StreamExecutionEnvironment.get_execution_environment()

# Create a data stream of words
data_stream = env.from_collection(["hello", "world", "hello", "flink", "world", "hello"])

# Define the KeyedProcessFunction for Word Count
class WordCount(KeyedProcessFunction):

    def open(self, runtime_context):
        # Initialize the ValueState to track the count for each word
        self.word_count_state = runtime_context.get_state(
            ValueStateDescriptor("word_count", Types.INT()))

    def process_element(self, value, ctx):
        # Retrieve the current count or initialize to 0
        current_count = self.word_count_state.value() or 0
        # Increment the count for the word
        current_count += 1
        # Update the state with the new count
        self.word_count_state.update(current_count)
        # Print the word and its updated count
        print(f"Word: {value}, Count: {current_count}")

# Apply KeyedProcessFunction
data_stream.key_by(lambda x: x).process(WordCount())

# Execute the Flink job
env.execute("Word Count with Keyed State")
```

#### **Output Example**:

```
Word: hello, Count: 1
Word: world, Count: 1
Word: hello, Count: 2
Word: flink, Count: 1
Word: world, Count: 2
Word: hello, Count: 3
```

---

### **Problem 2: Session Tracking with Keyed State**

#### Description:

Simulate a stream of user activity events and count the actions performed by each user. Use a `KeyedProcessFunction` to maintain a `ValueState` for session tracking.

```python
from pyflink.datastream import StreamExecutionEnvironment, KeyedProcessFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types

# Initialize the Stream Execution Environment
env = StreamExecutionEnvironment.get_execution_environment()

# Create a data stream of user activity events
# Format: (userID, action)
data_stream = env.from_collection([
    ("user1", "login"),
    ("user2", "browse"),
    ("user1", "logout"),
    ("user3", "login"),
    ("user2", "purchase"),
    ("user1", "login"),
    ("user3", "logout"),
])

# Define the KeyedProcessFunction for Session Tracking
class SessionTracker(KeyedProcessFunction):

    def open(self, runtime_context):
        # Initialize the ValueState to track action count for each user
        self.session_state = runtime_context.get_state(
            ValueStateDescriptor("session_count", Types.INT()))

    def process_element(self, value, ctx):
        # Retrieve the current session count or initialize to 0
        current_session_count = self.session_state.value() or 0
        # Increment the session count for the user
        current_session_count += 1
        # Update the state with the new session count
        self.session_state.update(current_session_count)
        # Print the user ID and updated session count
        print(f"User: {value[0]}, Session Count: {current_session_count}")

    def close(self):
        # This can be used to clean up resources or finalize any state
        print("Stream processing completed!")

# Apply KeyedProcessFunction
data_stream.key_by(lambda x: x[0]).process(SessionTracker())

# Execute the Flink job
env.execute("Session Tracking with Keyed State")
```

#### **Output Example**:

```
User: user1, Session Count: 1
User: user2, Session Count: 1
User: user1, Session Count: 2
User: user3, Session Count: 1
User: user2, Session Count: 2
User: user1, Session Count: 3
User: user3, Session Count: 2
```

---

### **Key Concepts in Both Problems**:

1. **Keyed State**:
    
    - Each key (e.g., word or user ID) maintains its own isolated state.
    - Enables efficient management of state in distributed systems.
2. **State Descriptor**:
    
    - Used to define the state type and name.
    - Example: `ValueStateDescriptor("state_name", Types.INT())`.
3. **State Management**:
    
    - `value()` retrieves the current state.
    - `update(value)` updates the state with a new value.
4. **Use Case**:
    
    - Word count is common in text processing or logging applications.
    - Session tracking is useful for user activity monitoring or analytics.

---

# 8.
![[Pasted image 20241223012714.png]]

### **Problem 3: Detecting Consecutive Failures with Keyed State**

#### **Description**:

Monitor a stream of machine logs, track consecutive `"FAIL"` statuses for each machine using `ValueState`, and raise an alert when a machine logs three consecutive `"FAIL"` statuses.

```python
from pyflink.datastream import StreamExecutionEnvironment, KeyedProcessFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types

# Initialize the Stream Execution Environment
env = StreamExecutionEnvironment.get_execution_environment()

# Create a data stream of machine logs
# Format: (machineID, status)
data_stream = env.from_collection([
    ("machine1", "OK"),
    ("machine1", "FAIL"),
    ("machine1", "FAIL"),
    ("machine1", "FAIL"),
    ("machine2", "OK"),
    ("machine2", "FAIL"),
    ("machine2", "OK"),
])

# Define the KeyedProcessFunction to detect consecutive failures
class ConsecutiveFailures(KeyedProcessFunction):

    def open(self, runtime_context):
        # Initialize the ValueState to track consecutive failure count for each machine
        self.failure_state = runtime_context.get_state(
            ValueStateDescriptor("consecutive_failures", Types.INT()))

    def process_element(self, value, ctx):
        # Retrieve the current failure count or initialize to 0
        current_failures = self.failure_state.value() or 0
        
        # Update failure count based on status
        if value[1] == "FAIL":
            current_failures += 1
            self.failure_state.update(current_failures)
        else:
            self.failure_state.update(0)
        
        # Raise an alert if there are 3 consecutive failures
        if current_failures >= 3:
            print(f"ALERT: Machine {value[0]} has logged 3 consecutive FAILs!")
        else:
            print(f"Machine {value[0]}, Current Fail Count: {current_failures}")

# Apply KeyedProcessFunction
data_stream.key_by(lambda x: x[0]).process(ConsecutiveFailures())

# Execute the Flink job
env.execute("Detecting Consecutive Failures with Keyed State")
```

#### **Output Example**:

```
Machine machine1, Current Fail Count: 0
Machine machine1, Current Fail Count: 1
Machine machine1, Current Fail Count: 2
ALERT: Machine machine1 has logged 3 consecutive FAILs!
Machine machine2, Current Fail Count: 0
Machine machine2, Current Fail Count: 1
Machine machine2, Current Fail Count: 0
```

---

### **Problem 4: Cumulative Sum with Keyed State**

#### **Description**:

Calculate the cumulative sum of numbers in a stream grouped by a key. Use `KeyedProcessFunction` to maintain a running total for each key and print the cumulative sum after processing every element.

```python
from pyflink.datastream import StreamExecutionEnvironment, KeyedProcessFunction
from pyflink.datastream.state import ValueStateDescriptor
from pyflink.common.typeinfo import Types

# Initialize the Stream Execution Environment
env = StreamExecutionEnvironment.get_execution_environment()

# Create a data stream of numbers grouped by a key
# Format: (key, value)
data_stream = env.from_collection([
    ("A", 10),
    ("B", 20),
    ("A", 15),
    ("B", 25),
    ("A", 5),
    ("B", 30),
])

# Define the KeyedProcessFunction to calculate cumulative sum
class CumulativeSum(KeyedProcessFunction):

    def open(self, runtime_context):
        # Initialize the ValueState to track cumulative sum for each key
        self.sum_state = runtime_context.get_state(
            ValueStateDescriptor("cumulative_sum", Types.INT()))

    def process_element(self, value, ctx):
        # Retrieve the current cumulative sum or initialize to 0
        current_sum = self.sum_state.value() or 0
        
        # Add the current value to the cumulative sum
        current_sum += value[1]
        self.sum_state.update(current_sum)
        
        # Print the cumulative sum for the key
        print(f"Key: {value[0]}, Cumulative Sum: {current_sum}")

# Apply KeyedProcessFunction
data_stream.key_by(lambda x: x[0]).process(CumulativeSum())

# Execute the Flink job
env.execute("Cumulative Sum with Keyed State")
```

#### **Output Example**:

```
Key: A, Cumulative Sum: 10
Key: B, Cumulative Sum: 20
Key: A, Cumulative Sum: 25
Key: B, Cumulative Sum: 45
Key: A, Cumulative Sum: 30
Key: B, Cumulative Sum: 75
```

---

### **Key Concepts in Both Problems**:

1. **Keyed State**:
    
    - Isolated state for each key in the stream (e.g., machineID or key).
    - `ValueState` used to store the failure count or cumulative sum.
2. **Fault Tolerance**:
    
    - State is stored persistently to handle failures in distributed systems.
3. **Use Cases**:
    
    - **Consecutive Failures**: Monitoring systems for alerts (e.g., machine health, network status).
    - **Cumulative Sum**: Aggregating data over a stream (e.g., financial transactions, event counters).
4. **Scalability**:
    
    - Keyed state enables parallel processing by dividing the stream by key.

<hr><hr><hr><hr>

<hr><hr><hr><hr>

<hr><hr><hr><hr>


# 9. Creating a keyedstream object

![[Pasted image 20241223014808.png]]







