
### Stepwise Regression

Stepwise regression is a method used in ==statistical modeling to select a subset of predictor variables== ==by adding or removing them iteratively based on their statistical significance.== It is used to identify the most important variables in a dataset and remove irrelevant ones, improving the model's predictive power and interpretability. Stepwise regression involves two main techniques:

1. **Forward Selection**: Starts with no predictors and adds variables one by one, based on a criterion (e.g., p-value, AIC, BIC), until adding new variables no longer improves the model significantly.
   
2. **Backward Elimination (Backward Selection)**: Starts with all potential predictors and removes the least significant ones one at a time, based on the same criterion, until removing variables no longer improves the model.

3. **Stepwise Selection**: A combination of both forward and backward selection where variables can be added or removed based on their significance during the process.

### 1. **Forward Selection**

Forward selection starts with an empty model (no predictors). It then adds one predictor at a time based on which variable improves the model the most (e.g., lowering the p-value, reducing AIC/BIC). It continues until no significant improvement is achieved by adding more variables.

#### Steps:
- Start with no variables.
- Add the variable that improves the model the most.
- Repeat until no further significant improvement is possible.

### 2. **Backward Selection**

Backward elimination starts with all predictors included in the model. It removes the least significant variable (the one with the highest p-value or increases AIC/BIC the least) at each step until no further variables can be removed without worsening the model.

#### Steps:
- Start with all variables.
- Remove the variable that contributes least to the model.
- Repeat until no further improvement is possible by removing variables.

### 3. **Stepwise Selection (Combined)**

Stepwise selection combines forward and backward selection. It starts with forward selection by adding variables and, at each step, checks if any of the previously added variables can be removed to improve the model.

---

### Example Code in Python (using `statsmodels` and `sklearn`)

Hereâ€™s how you can implement stepwise regression using forward and backward selection:

```python
import pandas as pd
import numpy as np
import statsmodels.api as sm
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# Load dataset
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = pd.Series(boston.target)

# Add intercept for statsmodels
X = sm.add_constant(X)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Forward Selection
def forward_selection(X, y):
    initial_features = []
    remaining_features = list(X.columns)
    selected_features = []
    current_score, best_new_score = float('inf'), float('inf')

    while remaining_features and current_score == best_new_score:
        scores_with_candidates = []
        for candidate in remaining_features:
            features = initial_features + [candidate]
            X_new = X[features]
            model = sm.OLS(y, X_new).fit()
            score = model.aic
            scores_with_candidates.append((score, candidate))

        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates[0]

        if current_score > best_new_score:
            remaining_features.remove(best_candidate)
            initial_features.append(best_candidate)
            selected_features.append(best_candidate)
            current_score = best_new_score

    return selected_features

# Backward Elimination
def backward_selection(X, y):
    features = list(X.columns)
    while len(features) > 0:
        X_new = X[features]
        model = sm.OLS(y, X_new).fit()
        pvalues = model.pvalues
        worst_pval = pvalues.max()  # get the largest p-value
        if worst_pval > 0.05:  # threshold for p-value
            worst_feature = pvalues.idxmax()
            features.remove(worst_feature)
        else:
            break

    return features

# Applying Forward Selection
print("Forward Selection Result:")
forward_selected_features = forward_selection(X_train, y_train)
print(forward_selected_features)

# Applying Backward Selection
print("\nBackward Selection Result:")
backward_selected_features = backward_selection(X_train, y_train)
print(backward_selected_features)
```

### Explanation of Code:

1. **Dataset**: This code uses the Boston housing dataset (`load_boston()`), which is a common regression dataset.
2. **Forward Selection**:
   - It starts with an empty model and iteratively adds the variable that improves the AIC score the most. AIC (Akaike Information Criterion) is a measure used for model comparison, where a lower AIC indicates a better model.
3. **Backward Selection**:
   - Starts with all variables and removes the one with the highest p-value iteratively. The process stops when all remaining variables are statistically significant (p-value < 0.05).

You can modify the stopping criteria (e.g., based on p-value, AIC, or BIC) based on your specific use case.

### Conclusion

- **Forward Selection** is useful when you start with no idea which variables are important.
- **Backward Elimination** works better when you have a large number of variables and want to eliminate irrelevant ones.
- **Stepwise Selection** combines both, making it flexible by adding and removing variables as necessary.