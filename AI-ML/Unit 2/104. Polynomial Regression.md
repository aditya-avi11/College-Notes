
**Polynomial Regression** is a type of regression model in machine learning that fits a nonlinear relationship between the independent variable \(x\) and the dependent variable \(y\). Unlike linear regression, which fits a straight line to the data, polynomial regression models the relationship as an nth-degree polynomial curve.

### Mathematical Representation
A polynomial regression model is expressed as:


$y = b_0 + b_1x + b_2x^2 + \dots + b_nx^n$


Where:
- y is the predicted output.
- $x$ is the input feature.
- $b_0, b_1, \dots, b_n$ are the coefficients of the model.
- \( n \) is the degree of the polynomial.

### When to Use Polynomial Regression?
- **Non-linearity**: When the data shows a non-linear relationship between the features and the target variable.
- **Overfitting**: High-degree polynomials can lead to overfitting, so regularization techniques might be needed.

### Steps for Polynomial Regression
1. Transform the original features into polynomial features (e.g., adding powers of \(x\)).
2. Apply linear regression to fit the transformed features.

### Example Code: Polynomial Regression in Python

Here’s a step-by-step implementation of Polynomial Regression using `scikit-learn`:

```python
# Importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split

# Sample dataset
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)  # Features (reshaped into a column vector)
y = np.array([1, 4, 9, 16, 25, 36, 49, 64, 81])           # Target variable (y = x^2)

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Step 1: Create polynomial features
poly = PolynomialFeatures(degree=2)  # Degree 2 polynomial (quadratic)
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

# Step 2: Fit Linear Regression to the transformed features
model = LinearRegression()
model.fit(X_poly_train, y_train)

# Step 3: Predict on test data
y_pred = model.predict(X_poly_test)

# Visualizing the results
plt.scatter(X, y, color='blue')  # Original data points
plt.plot(X, model.predict(poly.transform(X)), color='red')  # Polynomial regression curve
plt.title("Polynomial Regression")
plt.xlabel("X")
plt.ylabel("y")
plt.show()

# Output coefficients and intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)
```

### Explanation of Code:
1. **Dataset**: We create a simple dataset where $y = x^2$ to demonstrate polynomial regression.
2. **Polynomial Features**: The `PolynomialFeatures` class generates new features by taking powers of the original features. For example, with degree 2, it adds $x^2$ as a feature to the original x.
3. **Model**: We fit a linear regression model to these polynomial features.
4. **Visualization**: The plot shows the original data points and the fitted polynomial curve.

### Output:
- The red curve shows the polynomial regression model fitting the data points.
- The `print` statement will display the coefficients and intercept for the model.

This approach can be extended to higher-degree polynomials by increasing the degree in the `PolynomialFeatures(degree=n)` function. However, increasing the degree too much can lead to **overfitting**.


<hr><hr><hr>



In **Polynomial Regression with multiple independent variables**, the model is still based on polynomial features, but now there are more than one input feature. This means that instead of just \( x \), you have multiple features  $x_1, x_2, \dots, x_n$ , and the polynomial terms are generated for combinations of these features.

### Mathematical Representation:
For two independent variables $x_1$ and $\( x_2 \)$, a 2nd-degree polynomial regression model would be:


$y = b_0 + b_1x_1 + b_2x_2 + b_3x_1^2 + b_4x_2^2 + b_5x_1x_2$


This can be generalized to any number of independent variables and any degree of polynomial.

### Polynomial Features:
When working with multiple independent variables, the `PolynomialFeatures` class will generate all interaction terms and powers of the features up to the specified degree.

For example, for  $X = [x_1, x_2$]  and degree = 2, `PolynomialFeatures` will generate:

$[1, x_1, x_2, x_1^2, x_1x_2, x_2^2]$


### Example Code: Polynomial Regression with Multiple Independent Variables

Here’s how you can perform polynomial regression with multiple independent variables using Python's `scikit-learn`:

```python
# Importing necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split

# Sample dataset with two independent variables
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 8], [8, 9], [9, 10]])  # Two features: x1, x2
y = np.array([10, 14, 20, 28, 38, 50, 64, 80, 98])  # Target variable

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Step 1: Create polynomial features (degree 2 for both variables)
poly = PolynomialFeatures(degree=2)  # Generating 2nd-degree polynomial features
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

# Step 2: Fit Linear Regression to the transformed features
model = LinearRegression()
model.fit(X_poly_train, y_train)

# Step 3: Predict on test data
y_pred = model.predict(X_poly_test)

# Output the predicted values
print("Predicted values:", y_pred)
print("True values:", y_test)

# Output coefficients and intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Visualizing the results in 3D (only works for 2 independent variables, x1 and x2)
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Scatter plot of original data
ax.scatter(X[:, 0], X[:, 1], y, color='blue')

# Creating grid for visualization of the polynomial fit
x_surf, y_surf = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),
                             np.linspace(X[:, 1].min(), X[:, 1].max(), 100))

# Polynomial fit on the grid
z_surf = model.predict(poly.transform(np.c_[x_surf.ravel(), y_surf.ravel()]))
z_surf = z_surf.reshape(x_surf.shape)

# Plotting the surface
ax.plot_surface(x_surf, y_surf, z_surf, color='red', alpha=0.7)

ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('y')

plt.title("Polynomial Regression with Multiple Variables (2nd Degree)")
plt.show()
```

### Explanation of Code:
1. **Dataset**: We use two independent variables \( x_1 \) and \( x_2 \), and the dependent variable \( y \). Here, \( y \) is some arbitrary relationship with \( x_1 \) and \( x_2 \).
2. **Polynomial Features**: The `PolynomialFeatures(degree=2)` generates all polynomial combinations of the input features up to degree 2. This includes terms like  $x_1^2$ ,$x_2^2$ , and  $x_1x_2$ .
3. **Model**: We fit a linear regression model to the polynomial features.
4. **Visualization**: For multiple variables, if you have only 2 features, you can visualize the 3D surface plot of the polynomial fit (as shown in the 3D plot above).
5. **Prediction**: The model's predicted values are printed for the test set.

### Key Points:
- Polynomial regression can be extended to multiple features, and the interaction terms between features (like $x_1 \times x_2)$ are automatically generated.
- Increasing the degree of the polynomial can increase model complexity, so it’s important to avoid overfitting by selecting an appropriate degree or applying regularization.




