### Feature Scaling in Machine Learning

Feature scaling is the process of ==normalizing the range of independent variables== or features of data. In machine learning, feature scaling is essential to ensure that each feature contributes equally to the model, especially for algorithms that rely on distance calculations.

#### Common Feature Scaling Techniques:

1. **Min-Max Scaling (Normalization)**
    
    - Scales features to a fixed range, typically [0, 1].
    - Formula: $$X' = \frac{X - X_{min}}{X_{max} - X_{min}}$$
    - Preserves the relationships of data.
    - Sensitive to outliers.

		`from sklearn.preprocessing import MinMaxScaler`
		`import pandas as pd`
		
		`df = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})`
		`scaler = MinMaxScaler()`
		`df['scaled_feature'] = scaler.fit_transform(df[['feature']])`
		`print(df)`

2. **Standardization (Z-score Normalization)**

	- Scales features to have zero mean and unit variance.
	- Formula:
				 $X′= \frac{X - \mu}{\sigma}$
	- Less sensitive to outliers.
	- Suitable for normally distributed data.

		`from sklearn.preprocessing import StandardScaler`
		`import pandas as pd`
		
		`df = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})`
		`scaler = StandardScaler()`
		`df['standardized_feature'] = scaler.fit_transform(df[['feature']])`
		`print(df)`

3. **Max Abs Scaling**

	- Scales features by their maximum absolute value.
	- Formula: $X′ = \frac{X}{|X_{max}|}X′=∣Xmax​∣X​$
	- Useful for data with zero mean and different variances.

		`from sklearn.preprocessing import MinMaxScaler`
		`import pandas as pd`
		
		`df = pd.DataFrame({'feature': [10, 20, 30, 40, 50]})`
		`scaler = MinMaxScaler()`
		`df['scaled_feature'] = scaler.fit_transform(df[['feature']])`
		`print(df)`

4. Binarizer