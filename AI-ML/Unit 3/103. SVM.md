
https://medium.com/low-code-for-advanced-data-science/support-vector-machines-svm-an-intuitive-explanation-b084d6238106

Support Vector Machine (SVM) is a powerful supervised learning algorithm ==used for both classification and regression tasks==, but it is mostly known for its effectiveness in classification. The ==***main goal of SVM is to find a hyperplane that best separates the data points of different classes in a high-dimensional space.***==

### Key Concepts in SVM

1. **Hyperplane**:
   - In an SVM, the algorithm tries to ==find the optimal hyperplane that divides the data points into different classes==. In 2D, a hyperplane is simply a line, while in 3D, it is a plane. In higher dimensions, the hyperplane is a generalized concept.
   - The optimal hyperplane is the one that maximizes the margin between the classes.

2. **Margin**:
   - The margin is the distance between the hyperplane and the closest data points from each class, known as ==**support vectors**==.
   - SVM aims to maximize this margin to ensure better separation between the classes, which helps improve the model's generalization ability.

3. **Support Vectors**:
   - The data points that lie closest to the hyperplane and influence its position are called support vectors. These points are critical because the SVM's decision boundary is determined by them, rather than the other points further away from the hyperplane.

4. **Linear SVM**:
   - If the data is linearly separable, SVM can find a straight hyperplane that perfectly separates the two classes.
   - The decision boundary is defined by a linear equation, and the goal is to maximize the margin between the two classes.

5. **Non-linear SVM**:
   - Many real-world datasets are not linearly separable, meaning there is no straight hyperplane that can separate the classes. In such cases, SVM uses a technique called the **kernel trick** to map the data into a higher-dimensional space where it becomes linearly separable.

### Kernel Trick

The kernel trick ==allows SVM to handle non-linearly separable data by transforming it into a higher-dimensional space== without explicitly computing the coordinates of the transformed data. The algorithm operates in this transformed space but remains computationally efficient.

Some common kernel functions include:

1. **Linear Kernel**:
   - Used when the data is linearly separable.

2. **Polynomial Kernel**:
   - Used to separate data that is not linearly separable by introducing polynomial terms.
   
3. **Radial Basis Function (RBF) Kernel**:
   - One of the most commonly used kernels, the RBF kernel maps the data into an infinite-dimensional space. It’s particularly useful when there is no clear linear separation in the original feature space.

4. **Sigmoid Kernel**:
   - This kernel behaves like a neural network activation function and can also be used to map data into a higher dimension.

### ==Soft Margin vs Hard Margin==

- **Hard Margin**:
   - This approach assumes that the data is perfectly separable, and it finds a hyperplane that strictly separates the classes without any errors. However, this can lead to overfitting when dealing with noisy data or outliers.

- **Soft Margin**:
   - To deal with non-separable data and outliers, SVM introduces a soft margin, which allows for some misclassifications but aims to keep them minimal. The amount of misclassification allowed is controlled by a ==regularization parameter **C**==, where a smaller C allows more misclassifications, and a larger C penalizes misclassifications more strictly.

### SVM for Classification

In a classification problem, SVM separates the data into distinct classes. For example, in binary classification, the goal is to find a hyperplane that divides the two classes (positive and negative) with the largest margin.

- **Decision Boundary**: The boundary created by SVM defines where the predictions change from one class to another.
- **Prediction**: New data points are classified based on which side of the hyperplane they fall on.

### SVM for Regression (SVR)

While SVM is primarily used for classification, it can also be applied to regression tasks (called Support Vector Regression, SVR). In SVR, the goal is to predict a continuous value and create a margin of tolerance (epsilon) around the predicted value, allowing for some error. Only the points outside this margin influence the model.

### Advantages of SVM

1. **Effective in High-Dimensional Spaces**: SVM is well-suited for high-dimensional data where the number of features is large relative to the number of samples.
   
2. **Robust to Overfitting**: Due to the margin maximization principle, SVM tends to generalize well, especially when a proper regularization parameter is chosen.

3. **Versatile**: By using different kernels, SVM can handle both linear and non-linear problems effectively.

4. **Works Well with Small Datasets**: SVM can work effectively on small datasets where other algorithms may struggle to generalize.

### Disadvantages of SVM

1. **Computationally Intensive**: Training an SVM, especially with large datasets and complex kernels (like RBF), can be time-consuming and resource-intensive.

2. **Choosing the Right Kernel**: The performance of SVM highly depends on the choice of kernel, and selecting the appropriate kernel and tuning parameters like C and gamma can be tricky.

3. **Doesn’t Perform Well on Large Noisy Datasets**: SVM struggles with datasets that contain a lot of noise and overlapping classes, as it tries to maximize the margin for separation, which can lead to overfitting in such cases.

### Conclusion

SVM is a highly effective algorithm for both classification and regression, especially when dealing with high-dimensional data. By focusing on finding the optimal hyperplane with the maximum margin, SVM excels in tasks that require a clear decision boundary. However, it requires careful tuning of hyperparameters and kernel selection for best results.

<hr>
<hr>

### Linear vs Non-Linear Data:

**Linear Data**:
- **Definition**: Data that can be separated or classified using a straight line (in two dimensions) or a hyperplane (in higher dimensions).
- **Example**: If you have two categories of data (e.g., red dots and blue dots) that can be separated by drawing a straight line between them, then the data is linear.
- **Visual**: Think of a 2D plot where two distinct sets of points can be divided using a single straight line.

**Non-Linear Data**:
- **Definition**: Data that cannot be separated by a straight line. Instead, more complex boundaries (curved or irregular) are required to distinguish between different categories.
- **Example**: If you have points in two categories that are mixed together in such a way that no single straight line can separate them, then the data is non-linear.
- **Visual**: In a 2D space, the categories might overlap in such a way that a straight line won’t work, but a curved boundary might.

---

### Linear vs Non-Linear SVM:

**Linear SVM**:
- **Definition**: Linear SVM is used when the data is linearly separable. It attempts to find the **maximum-margin hyperplane** that best separates the data into different categories.
- **How it Works**: A straight line (or hyperplane in higher dimensions) is drawn to maximize the distance between the boundary and the closest points from each class, known as **support vectors**.
- **Application**: Suitable for datasets where classes can be divided by a straight line or hyperplane.

**Non-Linear SVM**:
- **Definition**: Non-Linear SVM is used when the data is not linearly separable. It uses **kernel functions** to transform the data into a higher-dimensional space where a linear boundary can be used.
- **How it Works**: It applies a **kernel trick** (such as polynomial, radial basis function (RBF), or sigmoid) to map the original data to a higher-dimensional feature space, making it easier to classify the data linearly in this new space.
- **Application**: Suitable for complex datasets where the classes cannot be separated by a straight line or hyperplane in the original space.

---

### Multi-Class Classification using Linear SVM:

SVMs are inherently **binary classifiers** (they distinguish between two classes). However, for multi-class classification, various strategies are employed to adapt SVMs:

1. **One-vs-Rest (OvR)**:
   - **How it Works**: For each class, the SVM learns a decision boundary to distinguish that class from all other classes combined. If there are *N* classes, it will create *N* different SVM models.
   - **Classification**: When classifying a new data point, all *N* classifiers are applied, and the one with the highest score (or margin) wins.
   - **Example**: If there are 3 classes (A, B, C), SVM creates 3 models: A vs (B + C), B vs (A + C), and C vs (A + B).

2. **One-vs-One (OvO)**:
   - **How it Works**: A separate SVM classifier is trained for every pair of classes. If there are *N* classes, the number of classifiers will be *N*(N-1)/2.
   - **Classification**: For a new data point, each classifier votes for one class, and the class with the most votes is selected as the final prediction.
   - **Example**: For 3 classes (A, B, C), the pairs would be A vs B, A vs C, and B vs C.

