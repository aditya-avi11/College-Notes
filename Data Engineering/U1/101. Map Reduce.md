Must Watch - [Map Reduce Concept and Use](https://www.youtube.com/watch?v=cHGaQz0E7AU)


![[Pasted image 20240808175047.png]]


![[Pasted image 20240808175144.png]]

![[Pasted image 20240808180113.png]]

![[Pasted image 20240808180220.png]]
![[Pasted image 20240808181824.png]]

MapReduce is a core component of the Hadoop ecosystem, designed for processing large data sets in parallel across a distributed cluster. It follows a programming model where the computation is divided into two main phases: **Map** and **Reduce**. This model allows for efficient, scalable, and fault-tolerant data processing.

### 1. **MapReduce Workflow Overview**

- **Input**: Large data set stored in HDFS.
- **Map Phase**: Processes input data and produces intermediate key-value pairs.
- **Shuffle and Sort**: Organizes and sorts the intermediate data by key.
- **Reduce Phase**: Aggregates the sorted data, producing the final output.
- **Output**: The result is written back to HDFS.

### 2. **Detailed Workflow**

#### **Step 1: Input Splitting**

- The input data is split into smaller chunks, typically 64 MB or 128 MB (configurable). Each chunk is processed independently.
- Each split is processed by a separate Map task, enabling parallel processing.

#### **Step 2: Map Phase**

- **Mapping Function**: The Map function is applied to each input split. It takes an input key-value pair and produces one or more intermediate key-value pairs.
- **Example**: In a word count program, the input could be a line of text, the key is the line number, and the value is the line content. The output from the Map function would be key-value pairs where the key is a word and the value is the number `1`.

#### **Step 3: Shuffle and Sort**

- **Shuffling**: After the Map phase, the intermediate key-value pairs are shuffled across the cluster so that all values associated with the same key are sent to the same Reduce task.
- **Sorting**: The intermediate data is then sorted by key to prepare it for the Reduce phase.

#### **Step 4: Reduce Phase**

- **Reducing Function**: The Reduce function takes the sorted key and all associated values and processes them to produce a final output.
- **Example**: Continuing the word count example, the Reduce function would sum all the values (which represent word counts) for each key (which represents a unique word) to produce the total count for each word.

#### **Step 5: Output**

- The output from the Reduce tasks is written back to HDFS, typically in multiple files, one per reducer.
- The output is often used as input for another MapReduce job or for analysis.

### 3. **Key Concepts in MapReduce**

- **Data Locality**: The Map tasks are usually executed on the nodes where the data is located (data locality), minimizing network congestion and speeding up processing.
- **Fault Tolerance**: If a task fails, Hadoop automatically retries the task on another node, using the same input data split.
- **Scalability**: MapReduce can process petabytes of data across thousands of nodes, making it highly scalable.
- **Parallelism**: The independent execution of Map and Reduce tasks allows Hadoop to efficiently utilize the clusterâ€™s resources.

### 4. **MapReduce Example: Word Count**

- **Input**: A large text file split into multiple blocks.
- **Map Phase**: For each line of text, the Mapper emits key-value pairs (word, 1).
- **Shuffle and Sort**: All key-value pairs are shuffled and grouped by word.
- **Reduce Phase**: For each word, the Reducer sums up the counts (values) to produce the total occurrences of each word.
- **Output**: The final output is a list of words with their respective counts.

### 5. **Advantages of MapReduce**

- **Simplicity**: Abstracts complex distributed processing, allowing developers to focus on the application logic.
- **Fault Tolerance**: Automatically handles failures during data processing.
- **Scalability**: Can scale out to handle vast amounts of data by adding more nodes to the cluster.
- **Data Locality Optimization**: Processes data where it is stored, reducing the need for data movement.

### 6. **Disadvantages of MapReduce**

- **Latency**: Not suitable for real-time data processing due to its batch-oriented nature.
- **Complexity in Joins**: Implementing complex joins and iterative algorithms can be cumbersome.
- **I/O Overhead**: The shuffle and sort phase can be I/O-intensive, especially with large data sets.

### **Summary**

MapReduce is a powerful model for processing large datasets in a distributed environment. By dividing tasks into a Map phase (for processing data) and a Reduce phase (for aggregating results), it allows Hadoop to process data in parallel, efficiently utilizing cluster resources while ensuring fault tolerance and scalability.
